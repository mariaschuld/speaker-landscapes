{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Clean data\n",
    "\n",
    "Running the following cells reads data of the form\n",
    "\n",
    "    {\"author\": \"name_speaker1\", \"quote\":\"quote1\"}\n",
    "    {\"author\": \"name_speaker2\", \"quote\": \"quote2\"}\n",
    "    ...\n",
    "\n",
    "from a json lines file called ``raw_data.jl`` and saves a text file called ``clean_data.txt`` of the form\n",
    "\n",
    "    agent_name_speaker1 cleaned_quote1 \n",
    "    agent_name_speaker2 cleaned_quote2 \n",
    "    ...\n",
    "    \n",
    "Cleaning includes lower casing, removing non-essential punctuation and making n-grams.\n",
    "\n",
    "The input file has to be in the same folder as this notebook.\n",
    "\n",
    "The code uses a few design decisions that make it work on large files; up to 5GB of text should not be a problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS\n",
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Remove punctuation, make lower case, and append speaker token to utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add some symbols to standard punctuation, because they have different ascii representation\n",
    "PUNCTUATION = string.punctuation.replace(\"_\", \"\") + \"“”’‘‚…–\"  \n",
    "\n",
    "# We remove # and @ signs to preserve them in tweets \n",
    "PUNCTUATION = PUNCTUATION.replace(\"#\", \"\").replace(\"@\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = open(\"raw_data.jl\", \"r\")\n",
    "\n",
    "# loop over each tweet; this is preferred \n",
    "# to loading into a dataframe when files are very big\n",
    "for idx, row in enumerate(data_loader):\n",
    "\n",
    "    try:\n",
    "        # retrieve the text\n",
    "        row_dict = json.loads(row)\n",
    "        text = row_dict[\"quote\"]\n",
    "\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        print(f\"Decoding problem in row {idx} with content <{row}>. Breaking here.\")\n",
    "        break\n",
    "\n",
    "    # append text with speaker token\n",
    "    text = \"agent_\" + str(row_dict[\"author\"]).strip().replace(\" \", \"_\") + \" \" + text\n",
    "\n",
    "    # remove punctuation\n",
    "    text = ''.join(char for word in text for char in word if char not in PUNCTUATION)\n",
    "\n",
    "    # make lower case\n",
    "    text = \" \".join([word.lower() for word in text.split()])\n",
    "\n",
    "    if text:\n",
    "        with open(\"clean_data.txt\", 'a+') as f:\n",
    "            f.write('%s\\n' % text)\n",
    "\n",
    "data_loader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Make n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making bigrams...\n",
      "...and trigrams...\n",
      "...done.\n"
     ]
    }
   ],
   "source": [
    "def sentence_generator(path):\n",
    "    \"\"\"Read sentences from disk one-by-one\"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            yield line.strip().split()\n",
    "\n",
    "print(\"Making bigrams...\")\n",
    "\n",
    "# make the model that builds bigrams\n",
    "gram_model = Phrases(sentence_generator(\"clean_data.txt\"),\n",
    "                     min_count=70,\n",
    "                     threshold=10,\n",
    "                     max_vocab_size=2000000,\n",
    "                     connector_words=ENGLISH_CONNECTOR_WORDS)\n",
    "gram_model.freeze()\n",
    "\n",
    "# write sentences with bigrams into temporary file\n",
    "with open(\"clean_data-temp.txt\", 'w') as f:\n",
    "    for sentence in sentence_generator(\"clean_data.txt\"):\n",
    "        new_sentence = gram_model[sentence]\n",
    "        new_sentence = \" \".join(new_sentence) + \"\\n\"\n",
    "        f.write(new_sentence)\n",
    "\n",
    "print(\"...and trigrams...\")\n",
    "\n",
    "# repeat procedure on processed text to get bigrams of words (including bigrams of bigrams)\n",
    "gram_model = Phrases(sentence_generator(\"clean_data-temp.txt\"),\n",
    "                     min_count=70,\n",
    "                     threshold=10,\n",
    "                     max_vocab_size=2000000,\n",
    "                     connector_words=ENGLISH_CONNECTOR_WORDS)\n",
    "\n",
    "gram_model.freeze()\n",
    " \n",
    "# overwrite original text with sentences with up to 4-grams\n",
    "with open(\"clean_data.txt\", 'w') as f:\n",
    "    for sentence in sentence_generator(\"clean_data-temp.txt\"):            \n",
    "        new_sentence = gram_model[sentence]\n",
    "        new_sentence = \" \".join(new_sentence) + \"\\n\"\n",
    "        f.write(new_sentence)\n",
    "\n",
    "    # delete temporary file\n",
    "    os.remove(\"clean_data-temp.txt\")\n",
    "    print(\"...done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "landscape_env",
   "language": "python",
   "name": "landscape_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
