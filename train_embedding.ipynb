{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the word embedding\n",
    "\n",
    "Running the cells below reads in a text file called ``clean_data.txt`` of the form\n",
    "\n",
    "    agent_name_speaker1 cleaned_quote1 \n",
    "    agent_name_speaker2 cleaned_quote2 \n",
    "    ...\n",
    "    \n",
    "and trains a word embedding from the text, using the gensim package. \n",
    "\n",
    "The word embedding is stored as ``word_embedding.emb`` using gensim's native model storing function, and can be loaded back into a gensim embedding via ``gensim.models.KeyedVectors.load(word_embedding.emb)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from random import seed, shuffle\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By changing these seeds, randomness in the loading and training process\n",
    "# will produce a different embedding each time.\n",
    "\n",
    "data_seed = 42 \n",
    "model_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define some components used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class is needed to print out the loss during training, which can be useful to monitor\n",
    "# the convergence behaviour: if the loss decreases significantly between two epochs, the \n",
    "# model may not be fully trained yet.\n",
    "\n",
    "class PrintLoss(CallbackAny2Vec):\n",
    "    \"\"\"Callback to print loss after each epoch.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 1\n",
    "        self.loss_to_be_subed = 0\n",
    "        self.log = \"\"\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        loss_now = loss - self.loss_to_be_subed\n",
    "        self.loss_to_be_subed = loss\n",
    "        print('Loss after epoch {}: {}'.format(self.epoch, loss_now))\n",
    "        self.log += 'Loss after epoch {}: {} \\n'.format(self.epoch, loss_now)\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class is a generator object. Each time it is called it returns a new line (i.e. data sample)\n",
    "# from the file stored under \"path_to_data\". Most of the logic is a trick to perform pseudo-randomisation for large \n",
    "# input files.\n",
    "\n",
    "class DataGenerator(object):\n",
    "    def __init__(self, path_to_data,\n",
    "                 share_data=1.,\n",
    "                 chunk_size=10000,\n",
    "                 random_buffer_size=100000,\n",
    "                 data_seed=data_seed):\n",
    "        \n",
    "        \"\"\"Iterator that loads lines from a (possibly large) file in a mildly randomised fashion. \n",
    "        \n",
    "        A buffer stores a set of lines from the text file. The buffer is shuffled, and the first chunk \n",
    "        of lines is returned (that is, one such line is yielded each time the generator is called). The buffer \n",
    "        is filled up again with fresh lines and shuffled. This continues until no lines are left to fill the \n",
    "        buffer with, at which point the remaining lines are returned.\n",
    "        \n",
    "        Args:\n",
    "            path_to_data (str): Full path to a data file with one preprocessed sentence/document per line.\n",
    "            share_data (float): How much data to subsample - useful for bootstrapping.\n",
    "            chunk_size (int): Return so many lines from the random buffer at once before filling it up again. Larger\n",
    "                chunk sizes speed up training, but decrease randomness.\n",
    "            random_buffer_size (int): Keep so many lines from the data file in a buffer which is shuffled before\n",
    "                returning the samples in a chunk. Higher values take more RAM but lead to more randomness\n",
    "                when sampling the data. A value equal to the number of all samples would lead to perfectly\n",
    "                random samples.\n",
    "            data_seed (int): Random seed for data loading.\n",
    "        \"\"\"\n",
    "        if chunk_size > random_buffer_size:\n",
    "            raise ValueError(\"Chunk size cannot be larger than the buffer size.\")\n",
    "\n",
    "        self.path_to_data = path_to_data\n",
    "        self.share_of_original_data = share_data\n",
    "        self.chunk_size = chunk_size\n",
    "        self.random_buffer_size = random_buffer_size\n",
    "        seed(data_seed)\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        # load initial buffer\n",
    "        buffer = []\n",
    "        with open(self.path_to_data, \"r\") as f:\n",
    "\n",
    "            reached_end = False\n",
    "\n",
    "            # fill buffer for the first time\n",
    "            for i in range(self.random_buffer_size):\n",
    "                line = f.readline().strip().split(\" \")\n",
    "                if not line:\n",
    "                    reached_end = True\n",
    "                    break\n",
    "                buffer.append(line)\n",
    "\n",
    "            while not reached_end:\n",
    "\n",
    "                # randomise the buffer\n",
    "                shuffle(buffer)\n",
    "\n",
    "                # remove and return chunk from buffer\n",
    "                for i in range(self.chunk_size):\n",
    "                    # separate non-bootstrap case here for speed\n",
    "                    if self.share_of_original_data == 1.0:\n",
    "                        yield buffer.pop(0)\n",
    "                    else:\n",
    "                        # randomly decide whether this line is in\n",
    "                        # the bootstrapped data\n",
    "                        if np.random.rand() > self.share_of_original_data:\n",
    "                            buffer.pop(0)\n",
    "                            continue\n",
    "                        else:\n",
    "                            yield buffer.pop(0)\n",
    "\n",
    "                # fill up the buffer with a fresh chunk\n",
    "                for i in range(self.chunk_size):\n",
    "                    line = f.readline()\n",
    "                    if not line:\n",
    "                        reached_end = True\n",
    "                        break\n",
    "                    else:\n",
    "                        buffer.append(line.strip().split(\" \"))\n",
    "\n",
    "            # if end of file has been reached\n",
    "            # yield all elements left in the buffer\n",
    "            # in random order\n",
    "            shuffle(buffer)\n",
    "            for el in buffer:\n",
    "                yield el"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training the embedding\n",
    "\n",
    "Training can now be done in a few lines. The hyperparameters we use were found by empirically optimising various embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:08,192 : INFO : collecting all words and their counts\n",
      "2022-07-05 16:56:08,795 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-07-05 16:56:09,173 : INFO : PROGRESS: at sentence #10000, processed 11977 words, keeping 1126 word types\n",
      "2022-07-05 16:56:09,180 : INFO : PROGRESS: at sentence #20000, processed 23826 words, keeping 1848 word types\n",
      "2022-07-05 16:56:09,188 : INFO : PROGRESS: at sentence #30000, processed 35872 words, keeping 2482 word types\n",
      "2022-07-05 16:56:09,198 : INFO : PROGRESS: at sentence #40000, processed 47465 words, keeping 2895 word types\n",
      "2022-07-05 16:56:09,207 : INFO : PROGRESS: at sentence #50000, processed 59645 words, keeping 3347 word types\n",
      "2022-07-05 16:56:09,218 : INFO : PROGRESS: at sentence #60000, processed 71582 words, keeping 3632 word types\n",
      "2022-07-05 16:56:09,230 : INFO : PROGRESS: at sentence #70000, processed 83310 words, keeping 3824 word types\n",
      "2022-07-05 16:56:09,241 : INFO : PROGRESS: at sentence #80000, processed 95078 words, keeping 4016 word types\n",
      "2022-07-05 16:56:09,254 : INFO : PROGRESS: at sentence #90000, processed 106969 words, keeping 4120 word types\n",
      "2022-07-05 16:56:09,288 : INFO : collected 4143 word types from a corpus of 119096 raw words and 100000 sentences\n",
      "2022-07-05 16:56:09,289 : INFO : Creating a fresh vocabulary\n",
      "2022-07-05 16:56:09,319 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 4143 unique words (100.00% of original 4143, drops 0)', 'datetime': '2022-07-05T16:56:09.318947', 'gensim': '4.2.0', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-121-generic-x86_64-with-glibc2.29', 'event': 'prepare_vocab'}\n",
      "2022-07-05 16:56:09,320 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 119096 word corpus (100.00% of original 119096, drops 0)', 'datetime': '2022-07-05T16:56:09.320457', 'gensim': '4.2.0', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-121-generic-x86_64-with-glibc2.29', 'event': 'prepare_vocab'}\n",
      "2022-07-05 16:56:09,366 : INFO : deleting the raw counts dictionary of 4143 items\n",
      "2022-07-05 16:56:09,368 : INFO : sample=0.001 downsamples 5 most-common words\n",
      "2022-07-05 16:56:09,369 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 22990.664619936273 word corpus (19.3%% of prior 119096)', 'datetime': '2022-07-05T16:56:09.369199', 'gensim': '4.2.0', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-121-generic-x86_64-with-glibc2.29', 'event': 'prepare_vocab'}\n",
      "2022-07-05 16:56:09,437 : INFO : estimated required memory for 4143 words and 250 dimensions: 10357500 bytes\n",
      "2022-07-05 16:56:09,438 : INFO : resetting layer weights\n",
      "2022-07-05 16:56:09,446 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-07-05T16:56:09.446040', 'gensim': '4.2.0', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-121-generic-x86_64-with-glibc2.29', 'event': 'build_vocab'}\n",
      "2022-07-05 16:56:09,446 : INFO : Word2Vec lifecycle event {'msg': 'training model with 8 workers on 4143 vocabulary and 250 features, using sg=1 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2022-07-05T16:56:09.446907', 'gensim': '4.2.0', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-121-generic-x86_64-with-glibc2.29', 'event': 'train'}\n",
      "2022-07-05 16:56:10,451 : INFO : EPOCH 0 - PROGRESS: at 66.72% examples, 15863 words/s, in_qsize 0, out_qsize 1\n",
      "2022-07-05 16:56:10,508 : INFO : EPOCH 0: training on 119096 raw words (22991 effective words) took 1.1s, 21727 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 1: 165646.484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:11,501 : INFO : EPOCH 1: training on 119096 raw words (23026 effective words) took 1.0s, 23284 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 2: 128048.828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:12,503 : INFO : EPOCH 2: training on 119096 raw words (22946 effective words) took 1.0s, 23017 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 3: 128010.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:13,512 : INFO : EPOCH 3 - PROGRESS: at 83.47% examples, 18897 words/s, in_qsize 2, out_qsize 1\n",
      "2022-07-05 16:56:13,519 : INFO : EPOCH 3: training on 119096 raw words (23059 effective words) took 1.0s, 22780 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 4: 100015.59375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:14,541 : INFO : EPOCH 4 - PROGRESS: at 92.50% examples, 20759 words/s, in_qsize 1, out_qsize 1\n",
      "2022-07-05 16:56:14,557 : INFO : EPOCH 4: training on 119096 raw words (22993 effective words) took 1.0s, 22278 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 5: 125120.90625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:15,566 : INFO : EPOCH 5 - PROGRESS: at 67.25% examples, 15298 words/s, in_qsize 0, out_qsize 0\n",
      "2022-07-05 16:56:15,622 : INFO : EPOCH 5: training on 119096 raw words (22945 effective words) took 1.1s, 21650 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 6: 125705.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:16,633 : INFO : EPOCH 6 - PROGRESS: at 58.50% examples, 13713 words/s, in_qsize 0, out_qsize 0\n",
      "2022-07-05 16:56:16,724 : INFO : EPOCH 6: training on 119096 raw words (22956 effective words) took 1.1s, 20957 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 7: 131705.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:17,755 : INFO : EPOCH 7 - PROGRESS: at 16.47% examples, 4038 words/s, in_qsize 0, out_qsize 0\n",
      "2022-07-05 16:56:17,908 : INFO : EPOCH 7: training on 119096 raw words (23025 effective words) took 1.2s, 19518 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 8: 116914.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:18,968 : INFO : EPOCH 8 - PROGRESS: at 33.72% examples, 7127 words/s, in_qsize 8, out_qsize 0\n",
      "2022-07-05 16:56:19,007 : INFO : EPOCH 8: training on 119096 raw words (22893 effective words) took 1.1s, 20916 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 9: 105253.1875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:20,015 : INFO : EPOCH 9 - PROGRESS: at 75.52% examples, 17372 words/s, in_qsize 9, out_qsize 0\n",
      "2022-07-05 16:56:20,062 : INFO : EPOCH 9: training on 119096 raw words (22980 effective words) took 1.0s, 21940 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10: 134527.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:21,081 : INFO : EPOCH 10 - PROGRESS: at 67.36% examples, 14992 words/s, in_qsize 1, out_qsize 0\n",
      "2022-07-05 16:56:21,144 : INFO : EPOCH 10: training on 119096 raw words (22948 effective words) took 1.1s, 21300 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 11: 125840.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:22,201 : INFO : EPOCH 11 - PROGRESS: at 33.84% examples, 7113 words/s, in_qsize 0, out_qsize 0\n",
      "2022-07-05 16:56:22,309 : INFO : EPOCH 11: training on 119096 raw words (23013 effective words) took 1.2s, 19841 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 12: 72768.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:23,331 : INFO : EPOCH 12 - PROGRESS: at 25.12% examples, 5808 words/s, in_qsize 0, out_qsize 0\n",
      "2022-07-05 16:56:23,507 : INFO : EPOCH 12: training on 119096 raw words (23066 effective words) took 1.2s, 19330 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 13: 78308.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:24,517 : INFO : EPOCH 13 - PROGRESS: at 17.08% examples, 3542 words/s, in_qsize 0, out_qsize 0\n",
      "2022-07-05 16:56:24,783 : INFO : EPOCH 13: training on 119096 raw words (22929 effective words) took 1.3s, 18051 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 14: 108873.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:25,797 : INFO : EPOCH 14 - PROGRESS: at 67.18% examples, 15283 words/s, in_qsize 0, out_qsize 0\n",
      "2022-07-05 16:56:25,859 : INFO : EPOCH 14: training on 119096 raw words (23013 effective words) took 1.1s, 21481 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 15: 87158.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:26,892 : INFO : EPOCH 15 - PROGRESS: at 49.86% examples, 10945 words/s, in_qsize 6, out_qsize 1\n",
      "2022-07-05 16:56:26,913 : INFO : EPOCH 15: training on 119096 raw words (23010 effective words) took 1.0s, 21976 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 16: 61607.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:27,956 : INFO : EPOCH 16 - PROGRESS: at 17.06% examples, 3470 words/s, in_qsize 0, out_qsize 0\n",
      "2022-07-05 16:56:28,129 : INFO : EPOCH 16: training on 119096 raw words (22931 effective words) took 1.2s, 19001 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 17: 82806.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:29,137 : INFO : EPOCH 17 - PROGRESS: at 50.46% examples, 11525 words/s, in_qsize 1, out_qsize 0\n",
      "2022-07-05 16:56:29,230 : INFO : EPOCH 17: training on 119096 raw words (23068 effective words) took 1.1s, 21036 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 18: 78353.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:30,235 : INFO : EPOCH 18 - PROGRESS: at 100.00% examples, 23033 words/s, in_qsize 0, out_qsize 1\n",
      "2022-07-05 16:56:30,236 : INFO : EPOCH 18: training on 119096 raw words (23047 effective words) took 1.0s, 23010 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 19: 77107.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:31,253 : INFO : EPOCH 19 - PROGRESS: at 67.04% examples, 15486 words/s, in_qsize 0, out_qsize 1\n",
      "2022-07-05 16:56:31,308 : INFO : EPOCH 19: training on 119096 raw words (23049 effective words) took 1.1s, 21663 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 20: 72864.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:32,344 : INFO : EPOCH 20 - PROGRESS: at 58.03% examples, 13723 words/s, in_qsize 0, out_qsize 0\n",
      "2022-07-05 16:56:32,426 : INFO : EPOCH 20: training on 119096 raw words (22994 effective words) took 1.1s, 20672 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 21: 48689.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:33,443 : INFO : EPOCH 21 - PROGRESS: at 51.09% examples, 10761 words/s, in_qsize 0, out_qsize 0\n",
      "2022-07-05 16:56:33,548 : INFO : EPOCH 21: training on 119096 raw words (22976 effective words) took 1.1s, 20588 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 22: 57216.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:34,601 : INFO : EPOCH 22 - PROGRESS: at 59.07% examples, 12679 words/s, in_qsize 5, out_qsize 1\n",
      "2022-07-05 16:56:34,651 : INFO : EPOCH 22: training on 119096 raw words (23111 effective words) took 1.1s, 21052 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 23: 55650.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:35,676 : INFO : EPOCH 23 - PROGRESS: at 25.73% examples, 5167 words/s, in_qsize 1, out_qsize 0\n",
      "2022-07-05 16:56:35,829 : INFO : EPOCH 23: training on 119096 raw words (22973 effective words) took 1.2s, 19582 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 24: 61906.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:36,835 : INFO : EPOCH 24 - PROGRESS: at 84.19% examples, 19073 words/s, in_qsize 2, out_qsize 1\n",
      "2022-07-05 16:56:36,861 : INFO : EPOCH 24: training on 119096 raw words (23004 effective words) took 1.0s, 22409 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 25: 66359.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:37,890 : INFO : EPOCH 25 - PROGRESS: at 58.67% examples, 13217 words/s, in_qsize 5, out_qsize 1\n",
      "2022-07-05 16:56:37,918 : INFO : EPOCH 25: training on 119096 raw words (22916 effective words) took 1.1s, 21784 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 26: 68062.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:38,952 : INFO : EPOCH 26 - PROGRESS: at 16.83% examples, 3685 words/s, in_qsize 0, out_qsize 0\n",
      "2022-07-05 16:56:39,127 : INFO : EPOCH 26: training on 119096 raw words (22989 effective words) took 1.2s, 19087 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 27: 51215.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:40,173 : INFO : EPOCH 27 - PROGRESS: at 58.23% examples, 13483 words/s, in_qsize 0, out_qsize 1\n",
      "2022-07-05 16:56:40,238 : INFO : EPOCH 27: training on 119096 raw words (22939 effective words) took 1.1s, 20855 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 28: 79514.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:41,260 : INFO : EPOCH 28 - PROGRESS: at 42.01% examples, 9554 words/s, in_qsize 1, out_qsize 0\n",
      "2022-07-05 16:56:41,371 : INFO : EPOCH 28: training on 119096 raw words (23077 effective words) took 1.1s, 20618 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 29: 59417.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 16:56:42,387 : INFO : EPOCH 29 - PROGRESS: at 25.18% examples, 5779 words/s, in_qsize 0, out_qsize 2\n",
      "2022-07-05 16:56:42,483 : INFO : EPOCH 29: training on 119096 raw words (23000 effective words) took 1.1s, 20782 effective words/s\n",
      "2022-07-05 16:56:42,484 : INFO : Word2Vec lifecycle event {'msg': 'training on 3572880 raw words (689867 effective words) took 33.0s, 20882 effective words/s', 'datetime': '2022-07-05T16:56:42.484709', 'gensim': '4.2.0', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-121-generic-x86_64-with-glibc2.29', 'event': 'train'}\n",
      "2022-07-05 16:56:42,485 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4143, vector_size=250, alpha=0.025>', 'datetime': '2022-07-05T16:56:42.485935', 'gensim': '4.2.0', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-121-generic-x86_64-with-glibc2.29', 'event': 'created'}\n",
      "/tmp/ipykernel_34925/2139498310.py:17: DeprecationWarning: Call to deprecated `init_sims` (Use fill_norms() instead. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.wv.init_sims()\n",
      "2022-07-05 16:56:42,493 : INFO : KeyedVectors lifecycle event {'fname_or_handle': 'word_embedding.emb', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-07-05T16:56:42.493360', 'gensim': '4.2.0', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-121-generic-x86_64-with-glibc2.29', 'event': 'saving'}\n",
      "2022-07-05 16:56:42,557 : INFO : saved word_embedding.emb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 30: 64538.75\n"
     ]
    }
   ],
   "source": [
    "training_generator = DataGenerator(\"clean_data.txt\")\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences = training_generator, \n",
    "    vector_size = 250, # number of dimensions that the word vectors will have\n",
    "    window = 10,  # maximum distance between the current and predicted word\n",
    "    sg = 1,  # use skip-gram (semantic learning) as training algorithm\n",
    "    workers = 8,  # number of threads for training the model\n",
    "    min_count = 1,  # ignores all words with total frequency lower than this  # CHANGE BACK TO 5 \n",
    "    sorted_vocab = 1, # sort the words in the resulting embedding\n",
    "    seed = model_seed, # use a random seed for reproducability\n",
    "    epochs = 30,  # number of times training goes through the data\n",
    "    compute_loss = True, # print the loss in each epoch\n",
    "    callbacks = [PrintLoss()])\n",
    "    \n",
    "# normalise the word vectors\n",
    "model.wv.init_sims()\n",
    "\n",
    "# extract and save embedding\n",
    "emb = model.wv\n",
    "emb.save(\"word_embedding.emb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "landscape_env",
   "language": "python",
   "name": "landscape_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
