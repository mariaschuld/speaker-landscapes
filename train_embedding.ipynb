{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the word embedding\n",
    "\n",
    "Running the cells below reads in a text file called \"clean_data.txt\" of the form\n",
    "\n",
    "    agent_name_speaker1 cleaned_quote1 \n",
    "    agent_name_speaker2 cleaned_quote2 \n",
    "    ...\n",
    "    \n",
    "and trains a word embedding from the text, using the gensim package. \n",
    "\n",
    "The word embedding is stored as \"word_embedding.emb\" using gensim's native model storing function, and can be loaded back into a gensim embedding via `gensim.models.KeyedVectors.load(word_embedding.emb)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from random import seed, shuffle\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By changing these seeds, randomness in the loading and training process\n",
    "# will produce a different embedding each time\n",
    "\n",
    "data_seed = 42 \n",
    "model_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define some components used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class is needed to print out the loss during training, which can be useful to monitor\n",
    "# the convergence behaviour: if the loss decreases significantly between two epochs, the \n",
    "# model may not be fully trained yet.\n",
    "\n",
    "class PrintLoss(CallbackAny2Vec):\n",
    "    \"\"\"Callback to print loss after each epoch.\n",
    "    Credits https://stackoverflow.com/questions/54888490/gensim-word2vec-print-log-loss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 1\n",
    "        self.loss_to_be_subed = 0\n",
    "        self.log = \"\"\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        loss_now = loss - self.loss_to_be_subed\n",
    "        self.loss_to_be_subed = loss\n",
    "        print('Loss after epoch {}: {}'.format(self.epoch, loss_now))\n",
    "        self.log += 'Loss after epoch {}: {} \\n'.format(self.epoch, loss_now)\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class is a generator object. Each time it is called it returns a new line (i.e. data sample)\n",
    "# from the file stored under \"path_to_data\". Most of the logic is a trick to perform pseudo-randomisation for large \n",
    "# input files.\n",
    "\n",
    "class DataGenerator(object):\n",
    "    def __init__(self, path_to_data,\n",
    "                 share_data=1.,\n",
    "                 chunk_size=10000,\n",
    "                 random_buffer_size=100000,\n",
    "                 data_seed=data_seed):\n",
    "        \n",
    "        \"\"\"Iterator that loads lines from a (possibly large) file in a mildly randomised fashion. \n",
    "        \n",
    "        We want to feed the data randomly to avoid training to depend on the order of the quotes in the \n",
    "        file. However, true randomisation is very costly when text files become larger than a few GB.\n",
    "        \n",
    "        Here, a buffer stores a set of lines from the text file. The buffer is shuffled, and the first chunk \n",
    "        of lines is returned (that is, one such line is yielded each time the generator is called). The buffer \n",
    "        is filled up again with fresh lines and shuffled. This continues until no lines are left to fill the \n",
    "        buffer with, at which point the remaining lines are returned.\n",
    "        \n",
    "        Args:\n",
    "            path_to_data (str): Full path to a data file with one preprocessed sentence/document per line.\n",
    "            share_of_original_data (float):  and picks each line with probability share_of_original_data, which\n",
    "                effectively results in a dataset with approx n_data*share_of_original_data samples\n",
    "            chunk_size (int): Return so many lines from the random buffer at once before filling it up again. Larger\n",
    "                chunk sizes speed up training, but decrease randomness.\n",
    "            random_buffer_size (int): Keep so many lines from the data file in a buffer which is shuffled before\n",
    "                returning the samples in a chunk. Higher values take more RAM but lead to more randomness\n",
    "                when sampling the data. A value equal to the number of all samples would lead to perfectly\n",
    "                random samples.\n",
    "        \"\"\"\n",
    "        if chunk_size > random_buffer_size:\n",
    "            raise ValueError(\"Chunk size cannot be larger than the buffer size.\")\n",
    "\n",
    "        self.path_to_data = path_to_data\n",
    "        self.share_of_original_data = share_data\n",
    "        self.chunk_size = chunk_size\n",
    "        self.random_buffer_size = random_buffer_size\n",
    "        seed(data_seed)\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        # load initial buffer\n",
    "        buffer = []\n",
    "        with open(self.path_to_data, \"r\") as f:\n",
    "\n",
    "            reached_end = False\n",
    "\n",
    "            # fill buffer for the first time\n",
    "            for i in range(self.random_buffer_size):\n",
    "                line = f.readline().strip().split(\" \")\n",
    "                if not line:\n",
    "                    reached_end = True\n",
    "                    break\n",
    "                buffer.append(line)\n",
    "\n",
    "            while not reached_end:\n",
    "\n",
    "                # randomise the buffer\n",
    "                shuffle(buffer)\n",
    "\n",
    "                # remove and return chunk from buffer\n",
    "                for i in range(self.chunk_size):\n",
    "                    # separate non-bootstrap case here for speed\n",
    "                    if self.share_of_original_data == 1.0:\n",
    "                        yield buffer.pop(0)\n",
    "                    else:\n",
    "                        # randomly decide whether this line is in\n",
    "                        # the bootstrapped data\n",
    "                        if np.random.rand() > self.share_of_original_data:\n",
    "                            # remove anyways\n",
    "                            buffer.pop(0)\n",
    "                            continue\n",
    "                        else:\n",
    "                            yield buffer.pop(0)\n",
    "\n",
    "                # fill up the buffer with a fresh chunk\n",
    "                for i in range(self.chunk_size):\n",
    "                    line = f.readline()\n",
    "                    if not line:\n",
    "                        reached_end = True\n",
    "                        break\n",
    "                    else:\n",
    "                        buffer.append(line.strip().split(\" \"))\n",
    "\n",
    "            # if end of file has been reached\n",
    "            # yield all elements left in the buffer\n",
    "            # in random order\n",
    "            shuffle(buffer)\n",
    "            for el in buffer:\n",
    "                yield el"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training the embedding\n",
    "\n",
    "Training can now be done in a few lines. The hyperparameters we use were found by empirically optimising various embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:04,053 : INFO : collecting all words and their counts\n",
      "2022-05-06 15:52:04,368 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-05-06 15:52:04,649 : INFO : PROGRESS: at sentence #10000, processed 11024 words, keeping 684 word types\n",
      "2022-05-06 15:52:04,656 : INFO : PROGRESS: at sentence #20000, processed 21965 words, keeping 1171 word types\n",
      "2022-05-06 15:52:04,664 : INFO : PROGRESS: at sentence #30000, processed 32880 words, keeping 1552 word types\n",
      "2022-05-06 15:52:04,672 : INFO : PROGRESS: at sentence #40000, processed 43804 words, keeping 1957 word types\n",
      "2022-05-06 15:52:04,679 : INFO : PROGRESS: at sentence #50000, processed 54805 words, keeping 2363 word types\n",
      "2022-05-06 15:52:04,688 : INFO : PROGRESS: at sentence #60000, processed 65802 words, keeping 2789 word types\n",
      "2022-05-06 15:52:04,696 : INFO : PROGRESS: at sentence #70000, processed 76744 words, keeping 3150 word types\n",
      "2022-05-06 15:52:04,706 : INFO : PROGRESS: at sentence #80000, processed 87404 words, keeping 3375 word types\n",
      "2022-05-06 15:52:04,717 : INFO : PROGRESS: at sentence #90000, processed 98367 words, keeping 3733 word types\n",
      "2022-05-06 15:52:04,742 : INFO : collected 4143 word types from a corpus of 109548 raw words and 100000 sentences\n",
      "2022-05-06 15:52:04,743 : INFO : Creating a fresh vocabulary\n",
      "2022-05-06 15:52:04,759 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 4143 unique words (100.00% of original 4143, drops 0)', 'datetime': '2022-05-06T15:52:04.759084', 'gensim': '4.2.0', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-109-generic-x86_64-with-glibc2.29', 'event': 'prepare_vocab'}\n",
      "2022-05-06 15:52:04,760 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 109548 word corpus (100.00% of original 109548, drops 0)', 'datetime': '2022-05-06T15:52:04.760024', 'gensim': '4.2.0', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-109-generic-x86_64-with-glibc2.29', 'event': 'prepare_vocab'}\n",
      "2022-05-06 15:52:04,789 : INFO : deleting the raw counts dictionary of 4143 items\n",
      "2022-05-06 15:52:04,790 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2022-05-06 15:52:04,790 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 13374.849682025575 word corpus (12.2%% of prior 109548)', 'datetime': '2022-05-06T15:52:04.790952', 'gensim': '4.2.0', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-109-generic-x86_64-with-glibc2.29', 'event': 'prepare_vocab'}\n",
      "2022-05-06 15:52:04,831 : INFO : estimated required memory for 4143 words and 250 dimensions: 10357500 bytes\n",
      "2022-05-06 15:52:04,831 : INFO : resetting layer weights\n",
      "2022-05-06 15:52:04,838 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-05-06T15:52:04.838893', 'gensim': '4.2.0', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-109-generic-x86_64-with-glibc2.29', 'event': 'build_vocab'}\n",
      "2022-05-06 15:52:04,839 : INFO : Word2Vec lifecycle event {'msg': 'training model with 8 workers on 4143 vocabulary and 250 features, using sg=1 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2022-05-06T15:52:04.839655', 'gensim': '4.2.0', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-109-generic-x86_64-with-glibc2.29', 'event': 'train'}\n",
      "2022-05-06 15:52:05,546 : INFO : EPOCH 0: training on 109548 raw words (13342 effective words) took 0.7s, 19021 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 1: 104621.3984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:06,288 : INFO : EPOCH 1: training on 109548 raw words (13378 effective words) took 0.7s, 18099 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 2: 93834.2109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:06,970 : INFO : EPOCH 2: training on 109548 raw words (13341 effective words) took 0.7s, 19674 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 3: 112565.390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:07,643 : INFO : EPOCH 3: training on 109548 raw words (13420 effective words) took 0.7s, 20156 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 4: 57122.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:08,337 : INFO : EPOCH 4: training on 109548 raw words (13423 effective words) took 0.7s, 19409 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 5: 67451.78125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:09,031 : INFO : EPOCH 5: training on 109548 raw words (13333 effective words) took 0.7s, 19306 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 6: 50952.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:09,717 : INFO : EPOCH 6: training on 109548 raw words (13408 effective words) took 0.7s, 19631 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 7: 61800.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:10,452 : INFO : EPOCH 7: training on 109548 raw words (13389 effective words) took 0.7s, 18297 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 8: 50515.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:11,143 : INFO : EPOCH 8: training on 109548 raw words (13393 effective words) took 0.7s, 19500 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 9: 53052.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:11,868 : INFO : EPOCH 9: training on 109548 raw words (13352 effective words) took 0.7s, 18504 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 10: 54611.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:12,632 : INFO : EPOCH 10: training on 109548 raw words (13356 effective words) took 0.8s, 17562 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 11: 96952.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:13,349 : INFO : EPOCH 11: training on 109548 raw words (13289 effective words) took 0.7s, 18622 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 12: 74864.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:14,204 : INFO : EPOCH 12: training on 109548 raw words (13395 effective words) took 0.9s, 15732 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 13: 71958.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:14,907 : INFO : EPOCH 13: training on 109548 raw words (13325 effective words) took 0.7s, 19047 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 14: 96457.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:15,588 : INFO : EPOCH 14: training on 109548 raw words (13437 effective words) took 0.7s, 19832 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 15: 75718.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:16,285 : INFO : EPOCH 15: training on 109548 raw words (13412 effective words) took 0.7s, 19363 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 16: 43618.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:16,958 : INFO : EPOCH 16: training on 109548 raw words (13345 effective words) took 0.7s, 19928 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 17: 82219.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:17,614 : INFO : EPOCH 17: training on 109548 raw words (13436 effective words) took 0.7s, 20632 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 18: 61734.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:18,260 : INFO : EPOCH 18: training on 109548 raw words (13314 effective words) took 0.6s, 20703 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 19: 53094.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:18,908 : INFO : EPOCH 19: training on 109548 raw words (13325 effective words) took 0.6s, 20662 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 20: 96475.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:19,537 : INFO : EPOCH 20: training on 109548 raw words (13322 effective words) took 0.6s, 21295 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 21: 58901.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:20,177 : INFO : EPOCH 21: training on 109548 raw words (13387 effective words) took 0.6s, 21025 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 22: 60677.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:20,798 : INFO : EPOCH 22: training on 109548 raw words (13330 effective words) took 0.6s, 21576 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 23: 91356.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:21,450 : INFO : EPOCH 23: training on 109548 raw words (13402 effective words) took 0.6s, 20688 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 24: 64995.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:22,094 : INFO : EPOCH 24: training on 109548 raw words (13444 effective words) took 0.6s, 20963 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 25: 69830.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:22,784 : INFO : EPOCH 25: training on 109548 raw words (13326 effective words) took 0.7s, 19410 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 26: 58318.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:23,421 : INFO : EPOCH 26: training on 109548 raw words (13345 effective words) took 0.6s, 21057 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 27: 45836.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:24,132 : INFO : EPOCH 27: training on 109548 raw words (13398 effective words) took 0.7s, 18943 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 28: 34906.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:24,782 : INFO : EPOCH 28: training on 109548 raw words (13366 effective words) took 0.6s, 20633 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 29: 49213.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 15:52:25,426 : INFO : EPOCH 29: training on 109548 raw words (13367 effective words) took 0.6s, 20855 effective words/s\n",
      "2022-05-06 15:52:25,427 : INFO : Word2Vec lifecycle event {'msg': 'training on 3286440 raw words (401100 effective words) took 20.6s, 19483 effective words/s', 'datetime': '2022-05-06T15:52:25.427292', 'gensim': '4.2.0', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-109-generic-x86_64-with-glibc2.29', 'event': 'train'}\n",
      "2022-05-06 15:52:25,428 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4143, vector_size=250, alpha=0.025>', 'datetime': '2022-05-06T15:52:25.428039', 'gensim': '4.2.0', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-109-generic-x86_64-with-glibc2.29', 'event': 'created'}\n",
      "/tmp/ipykernel_28705/2139498310.py:17: DeprecationWarning: Call to deprecated `init_sims` (Use fill_norms() instead. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.wv.init_sims()\n",
      "2022-05-06 15:52:25,432 : INFO : KeyedVectors lifecycle event {'fname_or_handle': 'word_embedding.emb', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-05-06T15:52:25.432391', 'gensim': '4.2.0', 'python': '3.8.10 (default, Mar 15 2022, 12:22:08) \\n[GCC 9.4.0]', 'platform': 'Linux-5.4.0-109-generic-x86_64-with-glibc2.29', 'event': 'saving'}\n",
      "2022-05-06 15:52:25,446 : INFO : saved word_embedding.emb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 30: 75453.875\n"
     ]
    }
   ],
   "source": [
    "training_generator = DataGenerator(\"clean_data.txt\")\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences = training_generator, \n",
    "    vector_size = 250, # number of dimensions that the word vectors will have\n",
    "    window = 10,  # maximum distance between the current and predicted word\n",
    "    sg = 1,  # use skip-gram (semantic learning) as training algorithm\n",
    "    workers = 8,  # number of threads for training the model\n",
    "    min_count = 1,  # ignores all words with total frequency lower than this  # CHANGE BACK TO 5 \n",
    "    sorted_vocab = 1, # sort the words in the resulting embedding\n",
    "    seed = model_seed, # use a random seed for reproducability\n",
    "    epochs = 30,  # number of times training goes through the data\n",
    "    compute_loss = True, # print the loss in each epoch\n",
    "    callbacks = [PrintLoss()])\n",
    "    \n",
    "# normalise the word vectors\n",
    "model.wv.init_sims()\n",
    "\n",
    "# extract and save embedding\n",
    "emb = model.wv\n",
    "emb.save(\"word_embedding.emb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "landscapes_env",
   "language": "python",
   "name": "landscapes_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
